---
title:          "Formalizing Robustness Against Character-Level Perturbations for Neural Network Language Models"
date:           2023-11-9
selected:       false
pub:            "International Conference on Formal Engineering Methods (ICFEM)"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-custom badge-success">Spotlight</span>'
pub_date:       "2023"

abstract: >-
  The remarkable success of neural networks has led to a growing demand for robustness verification and guarantee. However, the discrete nature of text data processed by language models presents challenges in measuring robustness, impeding verification efforts. To address this challenge, this work focuses on formalizing robustness specification against character-level perturbations for neural network language models. We introduce a key principle of three metrics, namely probability distribution, density, and diversity, for generalizing neural network language model perturbations and meanwhile, formulate the robustness specification against character-level perturbed text inputs. Based on the specification, we propose a novel approach to augment existing text datasets with specified perturbations, aiming to guide the robustness training of language models. Experimental results demonstrate that the training with our generated text datasets can enhance the overall robustness of the language model. Our contributions advance the field of neural network verification and provide a promising approach for handling robustness challenges in neural network language models.
# cover:          assets/images/covers/cover3.jpg
authors:
  - Zhongkui Ma
  - Xinguo Feng
  - Zihan Wang
  - Shuofeng Liu
  - Mengyao Ma
  - Hao Guan
  - Mark Huasong Meng 
links:
  paper: https://link.springer.com/chapter/10.1007/978-981-99-7584-6_7
  # Code: https://github.com/luost26/academic-homepage?
  # Unsplash: https://unsplash.com/photos/sliced-in-half-pineapple--_PLJZmHZzk
---
